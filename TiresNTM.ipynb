{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c6b517",
   "metadata": {},
   "source": [
    "## Resources\n",
    "https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_applying_machine_learning/ntm_20newsgroups_topic_modeling/ntm_20newsgroups_topic_model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388be94",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9610e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"comments_final.csv\")['Review']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79f567",
   "metadata": {},
   "source": [
    "## From Plain Text to Bag of Words (BOW)\n",
    "Input documents in the algorithm need to be vectors of integers representing word counts (BOW)\n",
    "\n",
    "Process:\n",
    "- tokenize documents (identify words and assign integer id)\n",
    "- count occurence of each token and form BOW vectors\n",
    "- use of stemming and lemmatization\n",
    "    - improves training and computing time(reduces vocab size)\n",
    "    - improves quality of learned topic-word probability matrices and inferred topic mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c00035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# --token counting--\n",
    "\n",
    "# rule: only consider words longer than 2 characters,\n",
    "# start with a letter and match the token_pattern\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [\n",
    "            self.wnl.lemmatize(t)\n",
    "            for t in word_tokenize(doc)\n",
    "            if len(t) >= 2 and re.match(\"[a-z].*\", t) and re.match(token_pattern, t)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e92f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and counting, this may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 1103\n",
      "Done. Time elapsed: 2.41s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# token counting performed while limiting vocab size to vocab_size\n",
    "vocab_size = 1103  # num extracted from vocab size derived\n",
    "print(\"Tokenizing and counting, this may take a few minutes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# performs token counting\n",
    "vectorizer = CountVectorizer(\n",
    "    input=\"content\",\n",
    "    analyzer=\"word\",\n",
    "    stop_words=\"english\",\n",
    "    tokenizer=LemmaTokenizer(),\n",
    "    max_features=vocab_size,\n",
    "    max_df=0.95,  # max doc frequency of 95%\n",
    "    min_df=2,     # min doc frequency of 2 docs\n",
    ")\n",
    "\n",
    "vectors = vectorizer.fit_transform(data)\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "print(\"vocab size:\", len(vocab_list))\n",
    "\n",
    "# random shuffle\n",
    "idx = np.arange(vectors.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "vectors = vectors[idx]\n",
    "\n",
    "print(\"Done. Time elapsed: {:.2f}s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f5a200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed short docs (<10 words)\n",
      "(352, 1103)\n"
     ]
    }
   ],
   "source": [
    "#removing documents shorter than 10 words\n",
    "threshold = 10\n",
    "\n",
    "vectors = vectors[\n",
    "    np.array(vectors.sum(axis=1) > threshold).reshape(\n",
    "        -1,\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"removed short docs (<{} words)\".format(threshold))\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e59049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> int64\n",
      "  (0, 62)\t2\n",
      "  (0, 345)\t1\n",
      "  (0, 729)\t1\n",
      "  (0, 978)\t2\n",
      "  (0, 194)\t1\n",
      "  (0, 497)\t1\n",
      "  (0, 833)\t2\n",
      "  (0, 581)\t1\n",
      "  (0, 578)\t1\n",
      "  (0, 278)\t1\n",
      "  (0, 877)\t1\n",
      "  (0, 769)\t1\n"
     ]
    }
   ],
   "source": [
    "# returns sparse matrices with integer elements\n",
    "print(type(vectors), vectors.dtype)\n",
    "print(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7b098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sparse\n",
    "\n",
    "#all parameters in NTM are np.float32\n",
    "#input data converted to np.float32\n",
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "print(type(vectors), vectors.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b9960",
   "metadata": {},
   "source": [
    "In modeling training, a training set, validation set, and test set are needed\n",
    "\n",
    "Training: set of data model is actually being trained on\n",
    "- periodically calculate scores on the validation set to validate performance of model on unseen data\n",
    "- stop training at optimal point to avoid over-training\n",
    "\n",
    "Validation set is needed in order to prevent over-training of NTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5066990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#80% of data used as training and rest for validation and test\n",
    "#validation used for training and test used for demonstrating model inference\n",
    "n_train = int(0.8 * vectors.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train_vectors = vectors[:n_train, :]\n",
    "test_vectors = vectors[n_train:, :]\n",
    "\n",
    "# further split test set into validation set (val_vectors) and test  set (test_vectors)\n",
    "n_test = test_vectors.shape[0]\n",
    "val_vectors = test_vectors[: n_test // 2, :]\n",
    "test_vectors = test_vectors[n_test // 2 :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2942ff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(281, 1103) (36, 1103) (35, 1103)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape, test_vectors.shape, val_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c29ac",
   "metadata": {},
   "source": [
    "## Store Data on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb34ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = 'sagemaker-studio-share'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fad80af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set location s3://sagemaker-studio-share/ntm/commentTires/train\n",
      "Validation set location s3://sagemaker-studio-share/ntm/commentTires/val\n",
      "Trained model will be saved at s3://sagemaker-studio-share/ntm/commentTires/output\n"
     ]
    }
   ],
   "source": [
    "prefix = \"ntm/commentTires\"\n",
    "\n",
    "train_prefix = os.path.join(prefix, \"train\")\n",
    "val_prefix = os.path.join(prefix, \"val\")\n",
    "output_prefix = os.path.join(prefix, \"output\")\n",
    "\n",
    "s3_train_data = os.path.join(\"s3://\", bucket, train_prefix)\n",
    "s3_val_data = os.path.join(\"s3://\", bucket, val_prefix)\n",
    "output_path = os.path.join(\"s3://\", bucket, output_prefix)\n",
    "print(\"Training set location\", s3_train_data)\n",
    "print(\"Validation set location\", s3_val_data)\n",
    "print(\"Trained model will be saved at\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35510425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecordIO Protobuf format: converted into binary representation of \n",
    "# 4-byte floats and loads into protobuf values field\n",
    "def split_convert_upload(sparray, bucket, prefix, fname_template=\"data_part{}.pbr\", n_parts=2):\n",
    "    import io\n",
    "    import boto3\n",
    "    import sagemaker.amazon.common as smac\n",
    "\n",
    "    chunk_size = sparray.shape[0] // n_parts\n",
    "    for i in range(n_parts):\n",
    "\n",
    "        # Calculate start and end indices\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size\n",
    "        if i + 1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "\n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "\n",
    "        # Upload to s3 location specified by bucket and prefix\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        boto3.resource(\"s3\").Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "        print(\"Uploaded data to s3://{}\".format(os.path.join(bucket, fname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb09e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data to s3://sagemaker-studio-share/ntm/commentTires/train/train_part0.pbr\n",
      "Uploaded data to s3://sagemaker-studio-share/ntm/commentTires/train/train_part1.pbr\n",
      "Uploaded data to s3://sagemaker-studio-share/ntm/commentTires/train/train_part2.pbr\n",
      "Uploaded data to s3://sagemaker-studio-share/ntm/commentTires/train/train_part3.pbr\n",
      "Uploaded data to s3://sagemaker-studio-share/ntm/commentTires/train/train_part4.pbr\n",
      "Uploaded data to s3://sagemaker-studio-share/ntm/commentTires/train/train_part5.pbr\n",
      "Uploaded data to s3://sagemaker-studio-share/ntm/commentTires/train/train_part6.pbr\n",
      "Uploaded data to s3://sagemaker-studio-share/ntm/commentTires/train/train_part7.pbr\n",
      "Uploaded data to s3://sagemaker-studio-share/ntm/commentTires/val/val_part0.pbr\n"
     ]
    }
   ],
   "source": [
    "split_convert_upload(\n",
    "    train_vectors, bucket=bucket, prefix=train_prefix, fname_template=\"train_part{}.pbr\", n_parts=8\n",
    ")\n",
    "split_convert_upload(\n",
    "    val_vectors, bucket=bucket, prefix=val_prefix, fname_template=\"val_part{}.pbr\", n_parts=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75148dea",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Configuring a sagemaker training job to use the prepared NTM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4385f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "container = retrieve(\"ntm\", boto3.Session().region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d816dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "#automatically chooses an alogirithm container based on current region\n",
    "sess = sagemaker.Session()\n",
    "ntm = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba296a2",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73464dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "ntm.set_hyperparameters(\n",
    "    num_topics=num_topics,\n",
    "    feature_dim=vocab_size,\n",
    "    mini_batch_size=128,\n",
    "    epochs=100,\n",
    "    num_patience_epochs=3,\n",
    "    tolerance=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a52ab640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speficifation of how training data and valiation data will be distributed\n",
    "# distribution values:\n",
    "    #FullyReplicated: all data files copied to all workers\n",
    "    #ShardedByS3Key: data files sharded to different workers\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "s3_train = TrainingInput(s3_train_data, distribution=\"ShardedByS3Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44defff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-30 07:10:57 Starting - Starting the training job...\n",
      "2022-03-30 07:11:23 Starting - Preparing the instances for trainingProfilerReport-1648624257: InProgress\n",
      ".........\n",
      "2022-03-30 07:12:50 Downloading - Downloading input data...\n",
      "2022-03-30 07:13:10 Training - Downloading the training image...\n",
      "2022-03-30 07:13:50 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:56 INFO 139801529890624 integration.py:636] worker started\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:56 INFO 139801529890624] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:56 INFO 139801529890624] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '100', 'feature_dim': '1103', 'mini_batch_size': '128', 'num_patience_epochs': '3', 'num_topics': '10', 'tolerance': '0.001'}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:56 INFO 139801529890624] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '128', 'epochs': '100', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '1103', 'num_topics': '10'}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:56 INFO 139801529890624] nvidia-smi: took 0.028 seconds to run.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:56 INFO 139801529890624] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:56 INFO 139801529890624] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:56 INFO 139801529890624] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-121-224.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-30-07-10-57-712', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:488267013278:training-job/ntm-2022-03-30-07-10-57-712', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-14ed0bd774aa7f6f776aac4679942e4bb2c0c2a0d517126ef52489aa4c404f70-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:56 INFO 139801529890624] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-121-224.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-30-07-10-57-712', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:488267013278:training-job/ntm-2022-03-30-07-10-57-712', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-14ed0bd774aa7f6f776aac4679942e4bb2c0c2a0d517126ef52489aa4c404f70-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'server', 'DMLC_PS_ROOT_URI': '10.0.120.153', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:56 INFO 139801529890624] Environment: {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-121-224.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-30-07-10-57-712', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:488267013278:training-job/ntm-2022-03-30-07-10-57-712', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-14ed0bd774aa7f6f776aac4679942e4bb2c0c2a0d517126ef52489aa4c404f70-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'worker', 'DMLC_PS_ROOT_URI': '10.0.120.153', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[35mProcess 34 is a shell:server.\u001b[0m\n",
      "\u001b[35mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:56 INFO 139801529890624] Using default worker.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:56 INFO 139801529890624] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:56.845] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:56 INFO 139801529890624] Initializing\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:56 INFO 139801529890624] None\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:56 INFO 139801529890624] vocab.txt\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:56 INFO 139801529890624] Vocab file is not provided\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:56 INFO 139801529890624] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:56 INFO 139801529890624] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344 integration.py:636] worker started\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '100', 'feature_dim': '1103', 'mini_batch_size': '128', 'num_patience_epochs': '3', 'num_topics': '10', 'tolerance': '0.001'}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '128', 'epochs': '100', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '1103', 'num_topics': '10'}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] nvidia-smi: took 0.028 seconds to run.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-120-153.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-30-07-10-57-712', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:488267013278:training-job/ntm-2022-03-30-07-10-57-712', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-14ed0bd774aa7f6f776aac4679942e4bb2c0c2a0d517126ef52489aa4c404f70-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-120-153.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-30-07-10-57-712', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:488267013278:training-job/ntm-2022-03-30-07-10-57-712', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-14ed0bd774aa7f6f776aac4679942e4bb2c0c2a0d517126ef52489aa4c404f70-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'scheduler', 'DMLC_PS_ROOT_URI': '10.0.120.153', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-120-153.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-30-07-10-57-712', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:488267013278:training-job/ntm-2022-03-30-07-10-57-712', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-14ed0bd774aa7f6f776aac4679942e4bb2c0c2a0d517126ef52489aa4c404f70-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-120-153.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-30-07-10-57-712', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:488267013278:training-job/ntm-2022-03-30-07-10-57-712', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-14ed0bd774aa7f6f776aac4679942e4bb2c0c2a0d517126ef52489aa4c404f70-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'server', 'DMLC_PS_ROOT_URI': '10.0.120.153', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] Environment: {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-120-153.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'ntm-2022-03-30-07-10-57-712', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:488267013278:training-job/ntm-2022-03-30-07-10-57-712', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-14ed0bd774aa7f6f776aac4679942e4bb2c0c2a0d517126ef52489aa4c404f70-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'worker', 'DMLC_PS_ROOT_URI': '10.0.120.153', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34mProcess 34 is a shell:scheduler.\u001b[0m\n",
      "\u001b[34mProcess 35 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] Using default worker.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:57.173] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] Initializing\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] None\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] vocab.txt\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] Vocab file is not provided\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:57 INFO 140134712129344] Create Store: dist_async\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.0324135, \"EndTime\": 1648624438.0324466, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.032] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 1196, \"num_examples\": 1, \"num_bytes\": 22612}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.139] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 106, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Finished training epoch 1 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 3.873632699251175\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.008185093087377027\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 3.8654476702213287\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 3.873632699251175\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #quality_metric: host=algo-2, epoch=1, train total_loss <loss>=3.873632699251175\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.146] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 1301, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.167] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 1.9002723693847656\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.0019721800927072763\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 1.8983001708984375\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 1.9002723693847656\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #validation_score (1): 1.9002723693847656\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Timing: train: 0.11s, val: 0.02s, epoch: 0.14s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #progress_metric: host=algo-2, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.032923, \"EndTime\": 1648624438.1699986, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Total Batches Seen\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1027.2567790795633 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.225] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 55, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Finished training epoch 2 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 3.8146638572216034\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.004524343414232135\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 3.810139685869217\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 3.8146638572216034\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #quality_metric: host=algo-2, epoch=2, train total_loss <loss>=3.8146638572216034\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.243] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 15, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 1.862228512763977\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.005739191081374884\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 1.8564893007278442\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 1.862228512763977\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #validation_score (2): 1.862228512763977\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Timing: train: 0.06s, val: 0.02s, epoch: 0.08s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #progress_metric: host=algo-2, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.170343, \"EndTime\": 1648624438.2502975, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 282.0, \"count\": 1, \"min\": 282, \"max\": 282}, \"Total Batches Seen\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1759.3766406854286 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.299] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 48, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Finished training epoch 3 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 3.7392358779907227\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.015037151519209146\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 3.724198579788208\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 3.7392358779907227\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #quality_metric: host=algo-2, epoch=3, train total_loss <loss>=3.7392358779907227\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.324] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 23, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 1.8201724290847778\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.019126826897263527\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 1.8010456562042236\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 1.8201724290847778\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #validation_score (3): 1.8201724290847778\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Timing: train: 0.05s, val: 0.03s, epoch: 0.08s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #progress_metric: host=algo-2, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.2506368, \"EndTime\": 1648624438.3296528, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 423.0, \"count\": 1, \"min\": 423, \"max\": 423}, \"Total Batches Seen\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1780.6829620797428 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.382] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 51, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Finished training epoch 4 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 3.652815580368042\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.044439506251364946\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 3.6083760261535645\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 3.652815580368042\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #quality_metric: host=algo-2, epoch=4, train total_loss <loss>=3.652815580368042\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.398] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 14, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 1.7968848943710327\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.028900712728500366\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 1.7679842710494995\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 1.7968848943710327\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #validation_score (4): 1.7968848943710327\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] patience losses:[1.9002723693847656, 1.862228512763977, 1.8201724290847778] min patience loss:1.8201724290847778 current loss:1.7968848943710327 absolute loss difference:0.023287534713745117\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Timing: train: 0.05s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #progress_metric: host=algo-2, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.3299675, \"EndTime\": 1648624438.403041, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 564.0, \"count\": 1, \"min\": 564, \"max\": 564}, \"Total Batches Seen\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1926.3866995876194 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.450] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 46, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Finished training epoch 5 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 3.5828075110912323\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.06737525667995214\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 3.5154323279857635\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 3.5828075110912323\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #quality_metric: host=algo-2, epoch=5, train total_loss <loss>=3.5828075110912323\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.468] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 15, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 1.7682380676269531\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.04306736961007118\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 1.7251707315444946\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 1.7682380676269531\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #validation_score (5): 1.7682380676269531\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] patience losses:[1.862228512763977, 1.8201724290847778, 1.7968848943710327] min patience loss:1.7968848943710327 current loss:1.7682380676269531 absolute loss difference:0.02864682674407959\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Timing: train: 0.05s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #progress_metric: host=algo-2, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.4032702, \"EndTime\": 1648624438.4737034, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 705.0, \"count\": 1, \"min\": 705, \"max\": 705}, \"Total Batches Seen\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1994.73441289265 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.517] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 39, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Finished training epoch 6 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 3.5259813964366913\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.0925929001532495\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 3.4333882927894592\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 3.5259813964366913\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #quality_metric: host=algo-2, epoch=6, train total_loss <loss>=3.5259813964366913\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.527] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 8, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 1.739000916481018\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.026387328281998634\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 1.712613582611084\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 1.739000916481018\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #validation_score (6): 1.739000916481018\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] patience losses:[1.8201724290847778, 1.7968848943710327, 1.7682380676269531] min patience loss:1.7682380676269531 current loss:1.739000916481018 absolute loss difference:0.02923715114593506\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Timing: train: 0.05s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #progress_metric: host=algo-2, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.4741547, \"EndTime\": 1648624438.5354009, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 846.0, \"count\": 1, \"min\": 846, \"max\": 846}, \"Total Batches Seen\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2295.296302046139 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.591] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 55, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Finished training epoch 7 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 3.476502686738968\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.06473061814904213\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 3.4117721617221832\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 3.476502686738968\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #quality_metric: host=algo-2, epoch=7, train total_loss <loss>=3.476502686738968\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.606] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 20, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 1.7231683731079102\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.05163080617785454\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 1.6715376377105713\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 1.7231683731079102\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #validation_score (7): 1.7231683731079102\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] patience losses:[1.7968848943710327, 1.7682380676269531, 1.739000916481018] min patience loss:1.739000916481018 current loss:1.7231683731079102 absolute loss difference:0.01583254337310791\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Timing: train: 0.06s, val: 0.02s, epoch: 0.08s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #progress_metric: host=algo-2, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.5357506, \"EndTime\": 1648624438.6126502, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 987.0, \"count\": 1, \"min\": 987, \"max\": 987}, \"Total Batches Seen\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1828.8154816205233 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.660] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 46, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Finished training epoch 8 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 3.450288712978363\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.10568614525254816\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 3.344602584838867\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 3.450288712978363\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #quality_metric: host=algo-2, epoch=8, train total_loss <loss>=3.450288712978363\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.673] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 23, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 1.7282313108444214\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.01005026325583458\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 1.7181810140609741\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 1.7282313108444214\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #validation_score (8): 1.7282313108444214\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] patience losses:[1.7682380676269531, 1.739000916481018, 1.7231683731079102] min patience loss:1.7231683731079102 current loss:1.7282313108444214 absolute loss difference:0.0050629377365112305\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Timing: train: 0.05s, val: 0.01s, epoch: 0.06s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #progress_metric: host=algo-2, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.6130278, \"EndTime\": 1648624438.6748009, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1128.0, \"count\": 1, \"min\": 1128, \"max\": 1128}, \"Total Batches Seen\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2277.4676766548696 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.726] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 50, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Finished training epoch 9 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 3.461828589439392\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.02948091086000204\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 3.4323477149009705\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 3.461828589439392\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #quality_metric: host=algo-2, epoch=9, train total_loss <loss>=3.461828589439392\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.744] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 26, \"duration\": 14, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 1.7203295230865479\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.046574972569942474\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 1.6737545728683472\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 1.7203295230865479\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #validation_score (9): 1.7203295230865479\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] patience losses:[1.739000916481018, 1.7231683731079102, 1.7282313108444214] min patience loss:1.7231683731079102 current loss:1.7203295230865479 absolute loss difference:0.0028388500213623047\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Timing: train: 0.05s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #progress_metric: host=algo-2, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.675051, \"EndTime\": 1648624438.750982, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1269.0, \"count\": 1, \"min\": 1269, \"max\": 1269}, \"Total Batches Seen\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1852.9734648030305 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.795] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 43, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Finished training epoch 10 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 3.4302971959114075\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.09470237069763243\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 3.3355948626995087\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 3.4302971959114075\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #quality_metric: host=algo-2, epoch=10, train total_loss <loss>=3.4302971959114075\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.0275574, \"EndTime\": 1648624438.0276053, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.027] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 870, \"num_examples\": 1, \"num_bytes\": 22676}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.158] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 130, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 1 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.840531826019287\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.008230659790569916\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.8323010206222534\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.840531826019287\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=3.840531826019287\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.166] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 992, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.177] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 11, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 1.897934913635254\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.001751543371938169\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 1.8961833715438843\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 1.897934913635254\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #validation_score (1): 1.897934913635254\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Timing: train: 0.14s, val: 0.01s, epoch: 0.15s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.0280473, \"EndTime\": 1648624438.1798506, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Total Batches Seen\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=921.5154720392208 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.219] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 39, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 2 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.778114825487137\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.004576910054311156\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.7735378742218018\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.778114825487137\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=3.778114825487137\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.238] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 17, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 1.862228512763977\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.005739191081374884\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 1.8564893007278442\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 1.862228512763977\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #validation_score (2): 1.862228512763977\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Timing: train: 0.04s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.18009, \"EndTime\": 1648624438.2443013, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 280.0, \"count\": 1, \"min\": 280, \"max\": 280}, \"Total Batches Seen\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=2175.4285820135224 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.294] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 49, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 3 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.710944503545761\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.01504384446889162\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.6959006786346436\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.710944503545761\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=3.710944503545761\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.320] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 25, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 1.8201724290847778\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.019126826897263527\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 1.8010456562042236\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 1.8201724290847778\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #validation_score (3): 1.8201724290847778\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Timing: train: 0.05s, val: 0.03s, epoch: 0.08s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.2446015, \"EndTime\": 1648624438.3292797, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 420.0, \"count\": 1, \"min\": 420, \"max\": 420}, \"Total Batches Seen\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1649.1952344307324 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.366] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 36, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 4 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.6255299150943756\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.04427714180201292\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.5812529027462006\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.6255299150943756\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=3.6255299150943756\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.382] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 14, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 1.7996150255203247\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.030453996732831\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 1.7691611051559448\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 1.7996150255203247\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #validation_score (4): 1.7996150255203247\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] patience losses:[1.897934913635254, 1.862228512763977, 1.8201724290847778] min patience loss:1.8201724290847778 current loss:1.7996150255203247 absolute loss difference:0.020557403564453125\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Timing: train: 0.04s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.3297093, \"EndTime\": 1648624438.3892236, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 560.0, \"count\": 1, \"min\": 560, \"max\": 560}, \"Total Batches Seen\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=2346.501282737786 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.423] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 33, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 5 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.5705077946186066\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.06669161096215248\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.5038161873817444\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.5705077946186066\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=3.5705077946186066\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.441] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 16, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 1.7704356908798218\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.04572783783078194\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 1.724707841873169\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 1.7704356908798218\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #validation_score (5): 1.7704356908798218\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] patience losses:[1.862228512763977, 1.8201724290847778, 1.7996150255203247] min patience loss:1.7996150255203247 current loss:1.7704356908798218 absolute loss difference:0.02917933464050293\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Timing: train: 0.03s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.3895106, \"EndTime\": 1648624438.445179, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 700.0, \"count\": 1, \"min\": 700, \"max\": 700}, \"Total Batches Seen\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=2508.0835796415577 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.480] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 33, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 6 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.5134919583797455\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.0964934783987701\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.4169984459877014\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.5134919583797455\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=3.5134919583797455\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.517] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 32, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 1.7399643659591675\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.02728613279759884\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 1.71267831325531\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 1.7399643659591675\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #validation_score (6): 1.7399643659591675\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] patience losses:[1.8201724290847778, 1.7996150255203247, 1.7704356908798218] min patience loss:1.7704356908798218 current loss:1.7399643659591675 absolute loss difference:0.030471324920654297\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Timing: train: 0.04s, val: 0.04s, epoch: 0.08s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.4455025, \"EndTime\": 1648624438.5214336, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 840.0, \"count\": 1, \"min\": 840, \"max\": 840}, \"Total Batches Seen\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1839.77992919134 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.557] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 35, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 7 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.4617918729782104\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.061276433523744345\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.4005154371261597\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.4617918729782104\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=3.4617918729782104\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.577] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 20, \"duration\": 18, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 1.7334942817687988\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.0665326789021492\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 1.666961669921875\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 1.7334942817687988\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #validation_score (7): 1.7334942817687988\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] patience losses:[1.7996150255203247, 1.7704356908798218, 1.7399643659591675] min patience loss:1.7399643659591675 current loss:1.7334942817687988 absolute loss difference:0.006470084190368652\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Timing: train: 0.04s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.521743, \"EndTime\": 1648624438.5827372, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 980.0, \"count\": 1, \"min\": 980, \"max\": 980}, \"Total Batches Seen\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=2289.8332936877778 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.626] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 42, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 8 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.455318421125412\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.1355556312482804\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.3197626769542694\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.455318421125412\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=3.455318421125412\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.650] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 23, \"duration\": 22, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 1.7311910390853882\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.009261159226298332\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 1.7219297885894775\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 1.7311910390853882\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #validation_score (8): 1.7311910390853882\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] patience losses:[1.7704356908798218, 1.7399643659591675, 1.7334942817687988] min patience loss:1.7334942817687988 current loss:1.7311910390853882 absolute loss difference:0.0023032426834106445\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Timing: train: 0.04s, val: 0.03s, epoch: 0.07s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.5830975, \"EndTime\": 1648624438.6562219, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1120.0, \"count\": 1, \"min\": 1120, \"max\": 1120}, \"Total Batches Seen\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1907.8645785951003 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.696] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 39, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 9 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.4442572593688965\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.022760024294257164\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.421497255563736\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.4442572593688965\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=3.4442572593688965\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.714] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 26, \"duration\": 17, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 1.727054238319397\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.05329561606049538\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 1.673758625984192\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 1.727054238319397\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #validation_score (9): 1.727054238319397\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] patience losses:[1.7399643659591675, 1.7334942817687988, 1.7311910390853882] min patience loss:1.7311910390853882 current loss:1.727054238319397 absolute loss difference:0.004136800765991211\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Timing: train: 0.04s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.656875, \"EndTime\": 1648624438.7198486, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1260.0, \"count\": 1, \"min\": 1260, \"max\": 1260}, \"Total Batches Seen\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=2217.2560066154897 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.758] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 37, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 10 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.4140985012054443\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.10793580813333392\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.3061626851558685\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.4140985012054443\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=3.4140985012054443\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.783] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 29, \"duration\": 22, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 1.7285293340682983\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.01061872486025095\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 1.7179105281829834\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 1.7285293340682983\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #validation_score (10): 1.7285293340682983\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] patience losses:[1.7334942817687988, 1.7311910390853882, 1.727054238319397] min patience loss:1.727054238319397 current loss:1.7285293340682983 absolute loss difference:0.0014750957489013672\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Timing: train: 0.04s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.720167, \"EndTime\": 1648624438.784536, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1400.0, \"count\": 1, \"min\": 1400, \"max\": 1400}, \"Total Batches Seen\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=2168.2953181715798 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.826] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 41, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 11 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.405538946390152\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.02503816829994321\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.3805007934570312\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.405538946390152\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=3.405538946390152\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.863] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 32, \"duration\": 35, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 1.7018680572509766\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.031696438789367676\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 1.6701716184616089\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 1.7018680572509766\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #validation_score (11): 1.7018680572509766\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] patience losses:[1.7311910390853882, 1.727054238319397, 1.7285293340682983] min patience loss:1.727054238319397 current loss:1.7018680572509766 absolute loss difference:0.02518618106842041\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Timing: train: 0.04s, val: 0.04s, epoch: 0.09s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.7848575, \"EndTime\": 1648624438.8722212, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1540.0, \"count\": 1, \"min\": 1540, \"max\": 1540}, \"Total Batches Seen\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1599.518838937436 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.827] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 29, \"duration\": 27, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 1.7091009616851807\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.022493086755275726\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 1.686607837677002\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 1.7091009616851807\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #validation_score (10): 1.7091009616851807\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] patience losses:[1.7231683731079102, 1.7282313108444214, 1.7203295230865479] min patience loss:1.7203295230865479 current loss:1.7091009616851807 absolute loss difference:0.011228561401367188\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Timing: train: 0.05s, val: 0.03s, epoch: 0.08s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #progress_metric: host=algo-2, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.7514157, \"EndTime\": 1648624438.8336275, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1410.0, \"count\": 1, \"min\": 1410, \"max\": 1410}, \"Total Batches Seen\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1710.5149620237055 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.873] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 38, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Finished training epoch 11 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 3.393923908472061\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.05011348193511367\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 3.3438103199005127\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 3.393923908472061\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #quality_metric: host=algo-2, epoch=11, train total_loss <loss>=3.393923908472061\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.903] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 32, \"duration\": 27, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 1.7014095783233643\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.03198816627264023\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 1.6694214344024658\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 1.7014095783233643\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #validation_score (11): 1.7014095783233643\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] patience losses:[1.7282313108444214, 1.7203295230865479, 1.7091009616851807] min patience loss:1.7091009616851807 current loss:1.7014095783233643 absolute loss difference:0.007691383361816406\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Timing: train: 0.04s, val: 0.03s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #progress_metric: host=algo-2, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.8339834, \"EndTime\": 1648624438.9091773, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1551.0, \"count\": 1, \"min\": 1551, \"max\": 1551}, \"Total Batches Seen\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1871.11826011023 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.952] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 43, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Finished training epoch 12 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 3.376379281282425\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.06657321169041097\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 3.309806078672409\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 3.376379281282425\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #quality_metric: host=algo-2, epoch=12, train total_loss <loss>=3.376379281282425\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:58.973] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 35, \"duration\": 19, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) total: 1.6861563920974731\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) kld: 0.0210771132260561\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) recons: 1.6650792360305786\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Loss (name: value) logppx: 1.6861563920974731\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #validation_score (12): 1.6861563920974731\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] patience losses:[1.7203295230865479, 1.7091009616851807, 1.7014095783233643] min patience loss:1.7014095783233643 current loss:1.6861563920974731 absolute loss difference:0.015253186225891113\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] Timing: train: 0.04s, val: 0.03s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #progress_metric: host=algo-2, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.9094656, \"EndTime\": 1648624438.981836, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1692.0, \"count\": 1, \"min\": 1692, \"max\": 1692}, \"Total Batches Seen\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1943.5048472858487 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:58 INFO 139801529890624] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.025] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 39, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 13 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.3755471110343933\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.047143780160695314\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.328403264284134\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.3755471110343933\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=13, train total_loss <loss>=3.3755471110343933\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.048] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 38, \"duration\": 21, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.6881139278411865\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.03235507383942604\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6557588577270508\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.6881139278411865\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (13): 1.6881139278411865\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.7091009616851807, 1.7014095783233643, 1.6861563920974731] min patience loss:1.6861563920974731 current loss:1.6881139278411865 absolute loss difference:0.001957535743713379\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.04s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624438.9822001, \"EndTime\": 1648624439.049816, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1833.0, \"count\": 1, \"min\": 1833, \"max\": 1833}, \"Total Batches Seen\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2081.4232308336645 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.087] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 37, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 14 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.3543923795223236\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.06491649243980646\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.289475977420807\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.3543923795223236\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=14, train total_loss <loss>=3.3543923795223236\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.109] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 41, \"duration\": 17, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.6807793378829956\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.01675875298678875\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6640206575393677\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.6807793378829956\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (14): 1.6807793378829956\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.7014095783233643, 1.6861563920974731, 1.6881139278411865] min patience loss:1.6861563920974731 current loss:1.6807793378829956 absolute loss difference:0.005377054214477539\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.04s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.0500047, \"EndTime\": 1648624439.1147795, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1974.0, \"count\": 1, \"min\": 1974, \"max\": 1974}, \"Total Batches Seen\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2171.8736971993917 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.157] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 41, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 15 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.3594777286052704\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.03726736828684807\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.3222104012966156\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.3594777286052704\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=15, train total_loss <loss>=3.3594777286052704\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.179] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 44, \"duration\": 20, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.6863632202148438\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.025713082402944565\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6606501340866089\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.6863632202148438\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (15): 1.6863632202148438\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.6861563920974731, 1.6881139278411865, 1.6807793378829956] min patience loss:1.6807793378829956 current loss:1.6863632202148438 absolute loss difference:0.0055838823318481445\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.04s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.1151056, \"EndTime\": 1648624439.1815293, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2115.0, \"count\": 1, \"min\": 2115, \"max\": 2115}, \"Total Batches Seen\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2116.803745409511 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.221] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 38, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 16 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.3441057205200195\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.05216325004585087\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.291942447423935\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.3441057205200195\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=16, train total_loss <loss>=3.3441057205200195\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.254] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 47, \"duration\": 32, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.6928292512893677\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.015011830255389214\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6778173446655273\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.6928292512893677\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (16): 1.6928292512893677\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.6881139278411865, 1.6807793378829956, 1.6863632202148438] min patience loss:1.6807793378829956 current loss:1.6928292512893677 absolute loss difference:0.01204991340637207\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.04s, val: 0.03s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.1818867, \"EndTime\": 1648624439.2565253, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2256.0, \"count\": 1, \"min\": 2256, \"max\": 2256}, \"Total Batches Seen\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1884.431732800566 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.303] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 45, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 17 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.3459460139274597\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.034276822581887245\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.3116692304611206\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.3459460139274597\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=17, train total_loss <loss>=3.3459460139274597\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.329] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 50, \"duration\": 25, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.687030553817749\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.021583614870905876\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6654468774795532\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.687030553817749\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (17): 1.687030553817749\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.6807793378829956, 1.6863632202148438, 1.6928292512893677] min patience loss:1.6807793378829956 current loss:1.687030553817749 absolute loss difference:0.006251215934753418\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.05s, val: 0.03s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.2568662, \"EndTime\": 1648624439.3312318, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2397.0, \"count\": 1, \"min\": 2397, \"max\": 2397}, \"Total Batches Seen\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1887.866590904738 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.380] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 48, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 18 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.330528110265732\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.04439809103496373\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.286130040884018\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.330528110265732\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=18, train total_loss <loss>=3.330528110265732\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.416] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 53, \"duration\": 33, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.6748658418655396\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.014038500376045704\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.660827398300171\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.6748658418655396\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (18): 1.6748658418655396\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.6863632202148438, 1.6928292512893677, 1.687030553817749] min patience loss:1.6863632202148438 current loss:1.6748658418655396 absolute loss difference:0.0114973783493042\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.05s, val: 0.04s, epoch: 0.09s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.331772, \"EndTime\": 1648624439.4230192, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2538.0, \"count\": 1, \"min\": 2538, \"max\": 2538}, \"Total Batches Seen\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1542.027550134413 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.467] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 43, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 19 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.3326923549175262\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.030635613948106766\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.302056699991226\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.3326923549175262\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=19, train total_loss <loss>=3.3326923549175262\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.488] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 56, \"duration\": 18, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.6974507570266724\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.013989601284265518\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6834611892700195\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.6974507570266724\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (19): 1.6974507570266724\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.6928292512893677, 1.687030553817749, 1.6748658418655396] min patience loss:1.6748658418655396 current loss:1.6974507570266724 absolute loss difference:0.022584915161132812\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.05s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.4233716, \"EndTime\": 1648624439.490828, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2679.0, \"count\": 1, \"min\": 2679, \"max\": 2679}, \"Total Batches Seen\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2081.5477730770044 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.536] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 44, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 20 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.3314971029758453\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.02957961428910494\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.301917552947998\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.3314971029758453\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=20, train total_loss <loss>=3.3314971029758453\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.551] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 59, \"duration\": 14, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.676715612411499\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.013706625439226627\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6630090475082397\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.676715612411499\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (20): 1.676715612411499\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.687030553817749, 1.6748658418655396, 1.6974507570266724] min patience loss:1.6748658418655396 current loss:1.676715612411499 absolute loss difference:0.0018497705459594727\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.05s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.4915524, \"EndTime\": 1648624439.5529842, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2820.0, \"count\": 1, \"min\": 2820, \"max\": 2820}, \"Total Batches Seen\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2285.786091957577 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.594] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 40, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 21 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.3217794001102448\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.029461215250194073\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.292318195104599\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.3217794001102448\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=21, train total_loss <loss>=3.3217794001102448\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.612] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 62, \"duration\": 16, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.6771700382232666\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.01804089918732643\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6591291427612305\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.6771700382232666\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (21): 1.6771700382232666\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.6748658418655396, 1.6974507570266724, 1.676715612411499] min patience loss:1.6748658418655396 current loss:1.6771700382232666 absolute loss difference:0.0023041963577270508\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.04s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.5532362, \"EndTime\": 1648624439.6135, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2961.0, \"count\": 1, \"min\": 2961, \"max\": 2961}, \"Total Batches Seen\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2334.0497754343314 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.652] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 38, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 22 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.326134294271469\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.037134536541998386\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.2889998853206635\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.326134294271469\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=22, train total_loss <loss>=3.326134294271469\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.668] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 65, \"duration\": 14, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.6733943223953247\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.01539977453649044\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6579945087432861\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.6733943223953247\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (22): 1.6733943223953247\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.6974507570266724, 1.676715612411499, 1.6771700382232666] min patience loss:1.676715612411499 current loss:1.6733943223953247 absolute loss difference:0.0033212900161743164\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.04s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.6137738, \"EndTime\": 1648624439.67463, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3102.0, \"count\": 1, \"min\": 3102, \"max\": 3102}, \"Total Batches Seen\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2310.4237778793527 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.729] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 48, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 23 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.3046815097332\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.03199812991078943\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.2726833820343018\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.3046815097332\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=23, train total_loss <loss>=3.3046815097332\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.744] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 68, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.6724119186401367\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.012619050219655037\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6597927808761597\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.6724119186401367\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (23): 1.6724119186401367\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.676715612411499, 1.6771700382232666, 1.6733943223953247] min patience loss:1.6733943223953247 current loss:1.6724119186401367 absolute loss difference:0.0009824037551879883\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.06s, val: 0.02s, epoch: 0.08s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.6749566, \"EndTime\": 1648624439.7542787, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3243.0, \"count\": 1, \"min\": 3243, \"max\": 3243}, \"Total Batches Seen\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1773.5060186767907 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.803] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 46, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 24 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.314026415348053\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.027321488363668323\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.286704897880554\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.314026415348053\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=24, train total_loss <loss>=3.314026415348053\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.911] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 37, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 12 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.355842351913452\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.06671015778556466\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.289132207632065\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.355842351913452\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=3.355842351913452\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.940] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 35, \"duration\": 27, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 1.6871470212936401\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.022673185914754868\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 1.6644738912582397\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 1.6871470212936401\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #validation_score (12): 1.6871470212936401\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] patience losses:[1.727054238319397, 1.7285293340682983, 1.7018680572509766] min patience loss:1.7018680572509766 current loss:1.6871470212936401 absolute loss difference:0.014721035957336426\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Timing: train: 0.04s, val: 0.03s, epoch: 0.07s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.8725343, \"EndTime\": 1648624438.9467716, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1680.0, \"count\": 1, \"min\": 1680, \"max\": 1680}, \"Total Batches Seen\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1881.4383713069083 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:58.994] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 46, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] # Finished training epoch 13 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) total: 3.360767960548401\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) kld: 0.04853557422757149\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) recons: 3.312232494354248\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] Loss (name: value) logppx: 3.360767960548401\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:58 INFO 140134712129344] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=3.360767960548401\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.016] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 38, \"duration\": 19, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.68950617313385\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.03420907258987427\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 1.6552971601486206\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 1.68950617313385\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #validation_score (13): 1.68950617313385\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] patience losses:[1.7285293340682983, 1.7018680572509766, 1.6871470212936401] min patience loss:1.6871470212936401 current loss:1.68950617313385 absolute loss difference:0.002359151840209961\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Timing: train: 0.05s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624438.9470904, \"EndTime\": 1648624439.0177686, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1820.0, \"count\": 1, \"min\": 1820, \"max\": 1820}, \"Total Batches Seen\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1976.9066528409492 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.049] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 31, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Finished training epoch 14 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 3.34707510471344\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.06912764208391309\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 3.2779476046562195\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 3.34707510471344\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=3.34707510471344\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.061] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 41, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.6818859577178955\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.01959056966006756\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 1.6622953414916992\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 1.6818859577178955\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #validation_score (14): 1.6818859577178955\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] patience losses:[1.7018680572509766, 1.6871470212936401, 1.68950617313385] min patience loss:1.6871470212936401 current loss:1.6818859577178955 absolute loss difference:0.005261063575744629\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Timing: train: 0.03s, val: 0.02s, epoch: 0.05s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624439.018045, \"EndTime\": 1648624439.0679123, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1960.0, \"count\": 1, \"min\": 1960, \"max\": 1960}, \"Total Batches Seen\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=2798.5881298821378 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.117] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 48, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Finished training epoch 15 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 3.3359692692756653\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.040682100458070636\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 3.295287162065506\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 3.3359692692756653\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=3.3359692692756653\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.136] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 44, \"duration\": 15, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.6825088262557983\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.018054036423563957\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 1.6644548177719116\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 1.6825088262557983\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #validation_score (15): 1.6825088262557983\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] patience losses:[1.6871470212936401, 1.68950617313385, 1.6818859577178955] min patience loss:1.6818859577178955 current loss:1.6825088262557983 absolute loss difference:0.000622868537902832\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Timing: train: 0.05s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624439.0682254, \"EndTime\": 1648624439.1383326, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2100.0, \"count\": 1, \"min\": 2100, \"max\": 2100}, \"Total Batches Seen\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1992.3947313061121 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.193] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 53, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Finished training epoch 16 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 3.3239735066890717\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.03961902251467109\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 3.284354329109192\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 3.3239735066890717\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=3.3239735066890717\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.213] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 47, \"duration\": 18, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.6940854787826538\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.013797791674733162\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 1.6802875995635986\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 1.6940854787826538\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #validation_score (16): 1.6940854787826538\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] patience losses:[1.68950617313385, 1.6818859577178955, 1.6825088262557983] min patience loss:1.6818859577178955 current loss:1.6940854787826538 absolute loss difference:0.0121995210647583\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Timing: train: 0.06s, val: 0.02s, epoch: 0.08s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624439.1386414, \"EndTime\": 1648624439.2151928, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2240.0, \"count\": 1, \"min\": 2240, \"max\": 2240}, \"Total Batches Seen\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1825.2032363646535 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.256] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 40, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Finished training epoch 17 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 3.337692469358444\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.030474469996988773\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 3.3072178959846497\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 3.337692469358444\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=3.337692469358444\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.277] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 50, \"duration\": 19, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.6864564418792725\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.02082757093012333\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 1.6656289100646973\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 1.6864564418792725\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #validation_score (17): 1.6864564418792725\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] patience losses:[1.6818859577178955, 1.6825088262557983, 1.6940854787826538] min patience loss:1.6818859577178955 current loss:1.6864564418792725 absolute loss difference:0.004570484161376953\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Timing: train: 0.04s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624439.2154758, \"EndTime\": 1648624439.281075, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2380.0, \"count\": 1, \"min\": 2380, \"max\": 2380}, \"Total Batches Seen\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=2120.737632094075 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.333] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 51, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Finished training epoch 18 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 3.3110640943050385\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.04398526577278972\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 3.2670786678791046\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 3.3110640943050385\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=3.3110640943050385\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.372] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 53, \"duration\": 37, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.674501895904541\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.015109025873243809\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 1.6593928337097168\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 1.674501895904541\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #validation_score (18): 1.674501895904541\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] patience losses:[1.6825088262557983, 1.6940854787826538, 1.6864564418792725] min patience loss:1.6825088262557983 current loss:1.674501895904541 absolute loss difference:0.008006930351257324\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Timing: train: 0.05s, val: 0.04s, epoch: 0.10s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624439.2818527, \"EndTime\": 1648624439.3780646, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2520.0, \"count\": 1, \"min\": 2520, \"max\": 2520}, \"Total Batches Seen\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1447.206425647383 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.426] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 46, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Finished training epoch 19 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 3.310954749584198\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.03203024622052908\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 3.278924435377121\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 3.310954749584198\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=3.310954749584198\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.441] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 56, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.6971977949142456\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.015965856611728668\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 1.68123197555542\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 1.6971977949142456\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #validation_score (19): 1.6971977949142456\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] patience losses:[1.6940854787826538, 1.6864564418792725, 1.674501895904541] min patience loss:1.674501895904541 current loss:1.6971977949142456 absolute loss difference:0.02269589900970459\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Timing: train: 0.05s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624439.378853, \"EndTime\": 1648624439.4450717, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2660.0, \"count\": 1, \"min\": 2660, \"max\": 2660}, \"Total Batches Seen\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=2106.730097909424 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.493] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 47, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Finished training epoch 20 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 3.317020982503891\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.03329145326279104\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 3.283729463815689\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 3.317020982503891\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=3.317020982503891\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.571] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 59, \"duration\": 74, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.6763499975204468\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.014293353073298931\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 1.662056565284729\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 1.6763499975204468\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #validation_score (20): 1.6763499975204468\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] patience losses:[1.6864564418792725, 1.674501895904541, 1.6971977949142456] min patience loss:1.674501895904541 current loss:1.6763499975204468 absolute loss difference:0.0018481016159057617\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Timing: train: 0.05s, val: 0.08s, epoch: 0.13s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624439.4454448, \"EndTime\": 1648624439.573169, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2800.0, \"count\": 1, \"min\": 2800, \"max\": 2800}, \"Total Batches Seen\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1094.6877674022344 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.614] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 40, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Finished training epoch 21 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 3.313379615545273\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.031254255678504705\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 3.2821254432201385\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 3.313379615545273\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=3.313379615545273\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.642] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 62, \"duration\": 26, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.6771113872528076\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.018217407166957855\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 1.6588940620422363\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 1.6771113872528076\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #validation_score (21): 1.6771113872528076\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] patience losses:[1.674501895904541, 1.6971977949142456, 1.6763499975204468] min patience loss:1.674501895904541 current loss:1.6771113872528076 absolute loss difference:0.0026094913482666016\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Timing: train: 0.04s, val: 0.03s, epoch: 0.07s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624439.5734806, \"EndTime\": 1648624439.6448026, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2940.0, \"count\": 1, \"min\": 2940, \"max\": 2940}, \"Total Batches Seen\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1958.1773556717533 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.690] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 44, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Finished training epoch 22 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 3.312756597995758\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.03647641954012215\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 3.276280164718628\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 3.312756597995758\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=3.312756597995758\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.716] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 65, \"duration\": 23, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.6722418069839478\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.013598003424704075\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 1.6586438417434692\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 1.6722418069839478\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #validation_score (22): 1.6722418069839478\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] patience losses:[1.6971977949142456, 1.6763499975204468, 1.6771113872528076] min patience loss:1.6763499975204468 current loss:1.6722418069839478 absolute loss difference:0.0041081905364990234\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Timing: train: 0.05s, val: 0.04s, epoch: 0.08s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624439.645117, \"EndTime\": 1648624439.728704, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3080.0, \"count\": 1, \"min\": 3080, \"max\": 3080}, \"Total Batches Seen\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1671.8986159632593 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.772] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 41, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Finished training epoch 23 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 3.2969166934490204\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.0285380098503083\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 3.2683786153793335\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 3.2969166934490204\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=3.2969166934490204\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.789] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 68, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.6732829809188843\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.015374725684523582\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 1.657908320426941\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 1.6732829809188843\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #validation_score (23): 1.6732829809188843\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] patience losses:[1.6763499975204468, 1.6771113872528076, 1.6722418069839478] min patience loss:1.6722418069839478 current loss:1.6732829809188843 absolute loss difference:0.0010411739349365234\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Timing: train: 0.05s, val: 0.02s, epoch: 0.06s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624439.7289848, \"EndTime\": 1648624439.7932723, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3220.0, \"count\": 1, \"min\": 3220, \"max\": 3220}, \"Total Batches Seen\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=2172.145317071908 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.853] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 58, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Finished training epoch 24 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 3.3099488615989685\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.03088174550794065\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 3.2790670692920685\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 3.3099488615989685\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=3.3099488615989685\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.844] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 71, \"duration\": 38, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.687916874885559\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.00935901328921318\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6785578727722168\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.687916874885559\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (24): 1.687916874885559\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.6771700382232666, 1.6733943223953247, 1.6724119186401367] min patience loss:1.6724119186401367 current loss:1.687916874885559 absolute loss difference:0.015504956245422363\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.05s, val: 0.04s, epoch: 0.09s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.754621, \"EndTime\": 1648624439.8459454, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3384.0, \"count\": 1, \"min\": 3384, \"max\": 3384}, \"Total Batches Seen\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1541.0029444412596 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.890] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 43, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 25 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.3102897107601166\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.02148819202557206\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.288801372051239\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.3102897107601166\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=25, train total_loss <loss>=3.3102897107601166\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.910] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 74, \"duration\": 17, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.6773791313171387\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.014036774635314941\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6633423566818237\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.6773791313171387\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (25): 1.6773791313171387\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.6733943223953247, 1.6724119186401367, 1.687916874885559] min patience loss:1.6724119186401367 current loss:1.6773791313171387 absolute loss difference:0.004967212677001953\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.05s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.8462374, \"EndTime\": 1648624439.9124863, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3525.0, \"count\": 1, \"min\": 3525, \"max\": 3525}, \"Total Batches Seen\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2122.4863584487175 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.967] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 54, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Finished training epoch 26 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 3.312977761030197\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.03049285465385765\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 3.2824848890304565\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 3.312977761030197\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #quality_metric: host=algo-2, epoch=26, train total_loss <loss>=3.312977761030197\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:13:59.983] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 77, \"duration\": 14, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) total: 1.6639958620071411\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) kld: 0.012157442048192024\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) recons: 1.6518384218215942\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Loss (name: value) logppx: 1.6639958620071411\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #validation_score (26): 1.6639958620071411\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] patience losses:[1.6724119186401367, 1.687916874885559, 1.6773791313171387] min patience loss:1.6724119186401367 current loss:1.6639958620071411 absolute loss difference:0.008416056632995605\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] Timing: train: 0.06s, val: 0.02s, epoch: 0.08s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #progress_metric: host=algo-2, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.91286, \"EndTime\": 1648624439.9883606, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3666.0, \"count\": 1, \"min\": 3666, \"max\": 3666}, \"Total Batches Seen\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1863.2952333542328 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:13:59 INFO 139801529890624] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:14:00.039] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 51, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] # Finished training epoch 27 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) total: 3.3151455521583557\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) kld: 0.026513627846725285\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) recons: 3.288631856441498\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) logppx: 3.3151455521583557\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #quality_metric: host=algo-2, epoch=27, train total_loss <loss>=3.3151455521583557\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:14:00.057] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 80, \"duration\": 16, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) total: 1.6767882108688354\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) kld: 0.009617919102311134\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) recons: 1.6671702861785889\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) logppx: 1.6767882108688354\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #validation_score (27): 1.6767882108688354\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] patience losses:[1.687916874885559, 1.6773791313171387, 1.6639958620071411] min patience loss:1.6639958620071411 current loss:1.6767882108688354 absolute loss difference:0.012792348861694336\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Timing: train: 0.05s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #progress_metric: host=algo-2, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624439.9886353, \"EndTime\": 1648624440.0617218, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3807.0, \"count\": 1, \"min\": 3807, \"max\": 3807}, \"Total Batches Seen\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=1917.5981712358748 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:14:00.109] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 46, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] # Finished training epoch 28 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) total: 3.309302866458893\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) kld: 0.021659540361724794\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) recons: 3.287643253803253\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) logppx: 3.309302866458893\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #quality_metric: host=algo-2, epoch=28, train total_loss <loss>=3.309302866458893\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:14:00.126] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 83, \"duration\": 14, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) total: 1.6782628297805786\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) kld: 0.009446008130908012\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) recons: 1.6688168048858643\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) logppx: 1.6782628297805786\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #validation_score (28): 1.6782628297805786\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] patience losses:[1.6773791313171387, 1.6639958620071411, 1.6767882108688354] min patience loss:1.6639958620071411 current loss:1.6782628297805786 absolute loss difference:0.0142669677734375\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Timing: train: 0.05s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #progress_metric: host=algo-2, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624440.0625012, \"EndTime\": 1648624440.1286283, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3948.0, \"count\": 1, \"min\": 3948, \"max\": 3948}, \"Total Batches Seen\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2125.7974982027317 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:14:00.170] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 41, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] # Finished training epoch 29 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) total: 3.3008519411087036\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) kld: 0.022389155812561512\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) recons: 3.278462827205658\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) logppx: 3.3008519411087036\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #quality_metric: host=algo-2, epoch=29, train total_loss <loss>=3.3008519411087036\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:14:00.194] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 86, \"duration\": 23, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) total: 1.6690937280654907\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) kld: 0.01519837137311697\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) recons: 1.653895378112793\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) logppx: 1.6690937280654907\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #validation_score (29): 1.6690937280654907\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] patience losses:[1.6639958620071411, 1.6767882108688354, 1.6782628297805786] min patience loss:1.6639958620071411 current loss:1.6690937280654907 absolute loss difference:0.005097866058349609\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Timing: train: 0.04s, val: 0.02s, epoch: 0.07s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #progress_metric: host=algo-2, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624440.1290145, \"EndTime\": 1648624440.1963325, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4089.0, \"count\": 1, \"min\": 4089, \"max\": 4089}, \"Total Batches Seen\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2088.523878304169 records/second\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] \u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:14:00.243] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 46, \"num_examples\": 2, \"num_bytes\": 25008}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] # Finished training epoch 30 on 141 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) total: 3.311278223991394\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) kld: 0.03264977957587689\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) recons: 3.278628557920456\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) logppx: 3.311278223991394\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #quality_metric: host=algo-2, epoch=30, train total_loss <loss>=3.311278223991394\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:14:00.258] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 89, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) total: 1.6915682554244995\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) kld: 0.003916420508176088\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) recons: 1.6876517534255981\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) logppx: 1.6915682554244995\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #validation_score (30): 1.6915682554244995\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] patience losses:[1.6767882108688354, 1.6782628297805786, 1.6690937280654907] min patience loss:1.6690937280654907 current loss:1.6915682554244995 absolute loss difference:0.02247452735900879\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Timing: train: 0.05s, val: 0.01s, epoch: 0.06s\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #progress_metric: host=algo-2, completed 100 % epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624440.196679, \"EndTime\": 1648624440.2611701, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4230.0, \"count\": 1, \"min\": 4230, \"max\": 4230}, \"Total Batches Seen\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Max Records Seen Between Resets\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #throughput_metric: host=algo-2, train throughput=2180.4980587786345 records/second\u001b[0m\n",
      "\u001b[35m[2022-03-30 07:14:00.484] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 92, \"duration\": 29, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) total: 1.6862655878067017\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) kld: 0.01122857816517353\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) recons: 1.675036907196045\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss (name: value) logppx: 1.6862655878067017\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] #quality_metric: host=algo-2, epoch=30, validation total_loss <loss>=1.6862655878067017\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Loss of server-side model: 1.6862655878067017\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Best model based on early stopping at epoch 26. Best loss: 1.6639958620071411\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Topics from epoch:final (num_topics:10) [, tu 0.23]:\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] 978 354 108 497 731 995 546 869 1048 480 729 1073 344 833 788 643 1010 448 1074 519\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] 978 108 546 833 344 869 354 995 729 1010 1048 497 480 1073 747 365 748 1099 1075 60\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] 978 108 354 869 497 1048 344 1004 448 1099 995 76 246 242 61 729 546 686 480 833\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] 978 354 869 344 995 108 497 1099 788 833 249 546 729 693 1004 163 62 756 242 853\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] 978 995 344 546 788 643 869 108 729 1004 833 354 1099 1010 448 61 1048 249 497 60\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] 978 108 354 833 344 995 546 97 1075 869 480 345 365 497 1048 1010 977 163 249 729\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] 978 108 1048 354 497 995 833 869 344 729 784 242 643 365 373 977 1004 60 62 693\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] 978 108 354 497 344 833 1048 995 977 480 60 97 546 1010 729 643 784 609 731 788\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] 978 833 354 344 108 643 60 788 869 747 1048 1099 995 497 345 546 63 519 1073 249\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] 978 108 833 344 995 1010 354 497 546 869 480 729 788 1048 1073 246 643 345 747 365\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Serializing model to /opt/ml/model/model_algo-2\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Saved checkpoint to \"/tmp/tmpih86vm74/state-0001.params\"\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624] Test data is not provided.\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1648624436.8355236, \"EndTime\": 1648624440.554184, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 1186.4852905273438, \"count\": 1, \"min\": 1186.4852905273438, \"max\": 1186.4852905273438}, \"epochs\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"model.score.time\": {\"sum\": 640.993595123291, \"count\": 31, \"min\": 9.12165641784668, \"max\": 39.4129753112793}, \"early_stop.time\": {\"sum\": 698.2924938201904, \"count\": 30, \"min\": 13.205289840698242, \"max\": 40.16232490539551}, \"update.time\": {\"sum\": 2210.6637954711914, \"count\": 30, \"min\": 60.09364128112793, \"max\": 136.83247566223145}, \"finalize.time\": {\"sum\": 95.95298767089844, \"count\": 1, \"min\": 95.95298767089844, \"max\": 95.95298767089844}, \"model.serialize.time\": {\"sum\": 3.984212875366211, \"count\": 1, \"min\": 3.984212875366211, \"max\": 3.984212875366211}, \"setuptime\": {\"sum\": 57.6176643371582, \"count\": 1, \"min\": 57.6176643371582, \"max\": 57.6176643371582}, \"totaltime\": {\"sum\": 3821.8400478363037, \"count\": 1, \"min\": 3821.8400478363037, \"max\": 3821.8400478363037}}}\u001b[0m\n",
      "\u001b[35m[03/30/2022 07:14:00 INFO 139801529890624 integration.py:636] worker closed\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.892] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 71, \"duration\": 36, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.6874513626098633\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.01148226484656334\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 1.675969123840332\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 1.6874513626098633\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #validation_score (24): 1.6874513626098633\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] patience losses:[1.6771113872528076, 1.6722418069839478, 1.6732829809188843] min patience loss:1.6722418069839478 current loss:1.6874513626098633 absolute loss difference:0.015209555625915527\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Timing: train: 0.06s, val: 0.04s, epoch: 0.10s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624439.7935965, \"EndTime\": 1648624439.8944385, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3360.0, \"count\": 1, \"min\": 3360, \"max\": 3360}, \"Total Batches Seen\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1385.4475787804718 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.973] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 78, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] # Finished training epoch 25 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 3.29786479473114\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) kld: 0.025746765080839396\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) recons: 3.272118002176285\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) logppx: 3.29786479473114\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=3.29786479473114\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:13:59.999] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 74, \"duration\": 24, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:13:59 INFO 140134712129344] Loss (name: value) total: 1.6751341819763184\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) kld: 0.011072876863181591\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) recons: 1.6640613079071045\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) logppx: 1.6751341819763184\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #validation_score (25): 1.6751341819763184\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] patience losses:[1.6722418069839478, 1.6732829809188843, 1.6874513626098633] min patience loss:1.6722418069839478 current loss:1.6751341819763184 absolute loss difference:0.0028923749923706055\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Timing: train: 0.08s, val: 0.03s, epoch: 0.11s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624439.8948004, \"EndTime\": 1648624440.001035, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3500.0, \"count\": 1, \"min\": 3500, \"max\": 3500}, \"Total Batches Seen\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1315.7657266033584 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:14:00.072] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 70, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] # Finished training epoch 26 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) total: 3.3077298998832703\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) kld: 0.023390634916722775\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) recons: 3.2843394577503204\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) logppx: 3.3077298998832703\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=3.3077298998832703\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:14:00.100] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 77, \"duration\": 23, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) total: 1.663641095161438\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) kld: 0.010157766751945019\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) recons: 1.6534833908081055\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) logppx: 1.663641095161438\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #validation_score (26): 1.663641095161438\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] patience losses:[1.6732829809188843, 1.6874513626098633, 1.6751341819763184] min patience loss:1.6732829809188843 current loss:1.663641095161438 absolute loss difference:0.009641885757446289\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Timing: train: 0.07s, val: 0.04s, epoch: 0.11s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624440.0013483, \"EndTime\": 1648624440.111823, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3640.0, \"count\": 1, \"min\": 3640, \"max\": 3640}, \"Total Batches Seen\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1265.1464866052768 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:14:00.166] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 53, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] # Finished training epoch 27 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) total: 3.309700667858124\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) kld: 0.023079979233443737\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) recons: 3.2866209149360657\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) logppx: 3.309700667858124\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=3.309700667858124\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:14:00.200] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 80, \"duration\": 31, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) total: 1.684177041053772\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) kld: 0.01519837137311697\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) recons: 1.6689786911010742\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) logppx: 1.684177041053772\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #validation_score (27): 1.684177041053772\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] patience losses:[1.6874513626098633, 1.6751341819763184, 1.663641095161438] min patience loss:1.663641095161438 current loss:1.684177041053772 absolute loss difference:0.020535945892333984\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Timing: train: 0.06s, val: 0.03s, epoch: 0.09s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624440.112206, \"EndTime\": 1648624440.2020442, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3780.0, \"count\": 1, \"min\": 3780, \"max\": 3780}, \"Total Batches Seen\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1554.8979867442347 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:14:00.267] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 65, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] # Finished training epoch 28 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) total: 3.311324894428253\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) kld: 0.029918106156401336\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) recons: 3.2814068496227264\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) logppx: 3.311324894428253\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=3.311324894428253\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:14:00.288] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 83, \"duration\": 17, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) total: 1.6821396350860596\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) kld: 0.004233458079397678\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) recons: 1.6779062747955322\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) logppx: 1.6821396350860596\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #validation_score (28): 1.6821396350860596\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] patience losses:[1.6751341819763184, 1.663641095161438, 1.684177041053772] min patience loss:1.663641095161438 current loss:1.6821396350860596 absolute loss difference:0.018498539924621582\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Timing: train: 0.07s, val: 0.02s, epoch: 0.09s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624440.2023883, \"EndTime\": 1648624440.2893112, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3920.0, \"count\": 1, \"min\": 3920, \"max\": 3920}, \"Total Batches Seen\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1607.3473409904632 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:14:00.338] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 48, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] # Finished training epoch 29 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) total: 3.3121065497398376\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) kld: 0.010489972308278084\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) recons: 3.3016165494918823\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) logppx: 3.3121065497398376\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=3.3121065497398376\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:14:00.352] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 86, \"duration\": 11, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) total: 1.6690123081207275\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) kld: 0.00841893907636404\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) recons: 1.6605933904647827\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) logppx: 1.6690123081207275\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #validation_score (29): 1.6690123081207275\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] patience losses:[1.663641095161438, 1.684177041053772, 1.6821396350860596] min patience loss:1.663641095161438 current loss:1.6690123081207275 absolute loss difference:0.005371212959289551\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Timing: train: 0.05s, val: 0.01s, epoch: 0.07s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624440.289669, \"EndTime\": 1648624440.3562005, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4060.0, \"count\": 1, \"min\": 4060, \"max\": 4060}, \"Total Batches Seen\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=2097.2718441053776 records/second\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] \u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:14:00.426] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 69, \"num_examples\": 2, \"num_bytes\": 24592}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] # Finished training epoch 30 on 140 examples from 2 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) total: 3.2883899211883545\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) kld: 0.0191289943177253\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) recons: 3.269260913133621\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) logppx: 3.2883899211883545\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=3.2883899211883545\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:14:00.438] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 89, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) total: 1.6846251487731934\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) kld: 0.009775788523256779\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) recons: 1.674849271774292\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) logppx: 1.6846251487731934\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #validation_score (30): 1.6846251487731934\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] patience losses:[1.684177041053772, 1.6821396350860596, 1.6690123081207275] min patience loss:1.6690123081207275 current loss:1.6846251487731934 absolute loss difference:0.01561284065246582\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Timing: train: 0.07s, val: 0.01s, epoch: 0.08s\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624440.356715, \"EndTime\": 1648624440.4414413, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4200.0, \"count\": 1, \"min\": 4200, \"max\": 4200}, \"Total Batches Seen\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Max Records Seen Between Resets\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Max Batches Seen Between Resets\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Batches Since Last Reset\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #throughput_metric: host=algo-1, train throughput=1649.3666313687268 records/second\u001b[0m\n",
      "\u001b[34m[2022-03-30 07:14:00.482] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 92, \"duration\": 23, \"num_examples\": 1, \"num_bytes\": 6512}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Finished scoring on 35 examples from 1 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) total: 1.6880217790603638\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) kld: 0.012237600982189178\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) recons: 1.6757841110229492\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss (name: value) logppx: 1.6880217790603638\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] #quality_metric: host=algo-1, epoch=30, validation total_loss <loss>=1.6880217790603638\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Loss of server-side model: 1.6880217790603638\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Best model based on early stopping at epoch 26. Best loss: 1.663641095161438\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Topics from epoch:final (num_topics:10) [, tu 0.23]:\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] 978 354 108 497 995 731 546 729 869 344 1048 480 643 1073 833 1010 788 1075 448 1074\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] 978 108 833 869 546 354 344 729 995 497 1010 1048 480 1073 747 60 748 1075 788 345\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] 978 108 869 1048 354 497 1099 344 448 1004 995 61 246 242 546 345 729 76 60 833\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] 978 354 869 344 995 108 497 1099 788 833 249 693 747 1004 546 729 473 508 977 609\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] 978 995 344 788 869 108 354 546 643 833 729 1004 1099 1010 1048 60 497 61 686 1074\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] 978 108 354 833 995 344 869 546 97 1075 365 480 977 345 1010 163 497 1048 729 473\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] 978 108 1048 995 354 497 869 344 833 242 643 365 373 729 784 1004 60 1073 788 693\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] 978 108 354 833 995 497 1048 344 977 480 60 788 729 643 546 1010 767 1099 609 97\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] 978 833 344 354 108 643 869 788 60 1048 995 747 497 345 546 249 707 1099 519 1010\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] 978 108 833 354 344 546 497 995 1010 729 869 480 643 1034 1048 747 345 788 246 365\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Saved checkpoint to \"/tmp/tmp83zsnu9y/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648624437.1567743, \"EndTime\": 1648624440.583283, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 852.7214527130127, \"count\": 1, \"min\": 852.7214527130127, \"max\": 852.7214527130127}, \"epochs\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"model.score.time\": {\"sum\": 773.1585502624512, \"count\": 31, \"min\": 10.716915130615234, \"max\": 75.1793384552002}, \"early_stop.time\": {\"sum\": 824.115514755249, \"count\": 30, \"min\": 13.239860534667969, \"max\": 75.64640045166016}, \"update.time\": {\"sum\": 2395.833730697632, \"count\": 30, \"min\": 49.658775329589844, \"max\": 151.60226821899414}, \"finalize.time\": {\"sum\": 130.22398948669434, \"count\": 1, \"min\": 130.22398948669434, \"max\": 130.22398948669434}, \"model.serialize.time\": {\"sum\": 2.9172897338867188, \"count\": 1, \"min\": 2.9172897338867188, \"max\": 2.9172897338867188}, \"setuptime\": {\"sum\": 65.62042236328125, \"count\": 1, \"min\": 65.62042236328125, \"max\": 65.62042236328125}, \"totaltime\": {\"sum\": 3542.2189235687256, \"count\": 1, \"min\": 3542.2189235687256, \"max\": 3542.2189235687256}}}\u001b[0m\n",
      "\u001b[34m[03/30/2022 07:14:00 INFO 140134712129344 integration.py:636] worker closed\u001b[0m\n",
      "\n",
      "2022-03-30 07:14:31 Uploading - Uploading generated training model\n",
      "2022-03-30 07:14:31 Completed - Training job completed\n",
      "Training seconds: 196\n",
      "Billable seconds: 196\n"
     ]
    }
   ],
   "source": [
    "# training process\n",
    "ntm.fit({\"train\": s3_train, \"validation\": s3_val_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "464e11d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: ntm-2022-03-30-07-10-57-712\n"
     ]
    }
   ],
   "source": [
    "print(\"Training job name: {}\".format(ntm.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c22dc9",
   "metadata": {},
   "source": [
    "## Model Hosting and Inference\n",
    "Using the model to perform inference on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8d4ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "ntm_predictor = ntm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37ff8d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: ntm-2022-03-30-07-14-43-302\n"
     ]
    }
   ],
   "source": [
    "print(\"Endpoint name: {}\".format(ntm_predictor.endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5b36016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'topic_weights': [0.1058430746, 0.1014523357, 0.0904826298, 0.095162861, 0.0985653922, 0.1119816527, 0.0958334953, 0.0774569064, 0.0908701196, 0.1323515326]}, {'topic_weights': [0.1046361849, 0.1003728062, 0.0933218226, 0.0963502377, 0.0981435105, 0.1086161882, 0.0976566747, 0.0840893239, 0.0935864151, 0.1232268512]}, {'topic_weights': [0.1054099873, 0.1011907309, 0.0909428373, 0.0953805298, 0.098705247, 0.1111631766, 0.0962012932, 0.0789502114, 0.0916484147, 0.1304075271]}, {'topic_weights': [0.1032081693, 0.0998822525, 0.0960021019, 0.0973289162, 0.0975634903, 0.1057101116, 0.0993679985, 0.0902152658, 0.0957135782, 0.1150081009]}, {'topic_weights': [0.1039087698, 0.0999538377, 0.0947371349, 0.0969901085, 0.0976432413, 0.1071557477, 0.0987916589, 0.0878592134, 0.0948566198, 0.1181037128]}, {'topic_weights': [0.1060861126, 0.1013164222, 0.0907030106, 0.0950350687, 0.0984542891, 0.1116873473, 0.0959659964, 0.0782032907, 0.0909786075, 0.131569922]}, {'topic_weights': [0.1030053645, 0.0997906178, 0.0961809531, 0.0974606425, 0.0976333097, 0.1053110138, 0.0994244814, 0.0910059288, 0.096265845, 0.1139218137]}, {'topic_weights': [0.1050562337, 0.1006817147, 0.0928574577, 0.0960784554, 0.0979526863, 0.1093731523, 0.0975761339, 0.0832466632, 0.0928208232, 0.1243567243]}, {'topic_weights': [0.1042312086, 0.1002227738, 0.094124645, 0.09675318, 0.0977495983, 0.1079208255, 0.0983715653, 0.0862504244, 0.0941884294, 0.1201872975]}, {'topic_weights': [0.1037845314, 0.0999689251, 0.094912, 0.0970652029, 0.0977064893, 0.1068806127, 0.0988146812, 0.0880399868, 0.0950548351, 0.1177726984]}]}\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array(test_vectors.todense())\n",
    "results = ntm_predictor.predict(test_data[:10])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b4b36fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10584307 0.10145234 0.09048263 0.09516286 0.09856539 0.11198165\n",
      "  0.0958335  0.07745691 0.09087012 0.13235153]\n",
      " [0.10463618 0.10037281 0.09332182 0.09635024 0.09814351 0.10861619\n",
      "  0.09765667 0.08408932 0.09358642 0.12322685]\n",
      " [0.10540999 0.10119073 0.09094284 0.09538053 0.09870525 0.11116318\n",
      "  0.09620129 0.07895021 0.09164841 0.13040753]\n",
      " [0.10320817 0.09988225 0.0960021  0.09732892 0.09756349 0.10571011\n",
      "  0.099368   0.09021527 0.09571358 0.1150081 ]\n",
      " [0.10390877 0.09995384 0.09473713 0.09699011 0.09764324 0.10715575\n",
      "  0.09879166 0.08785921 0.09485662 0.11810371]\n",
      " [0.10608611 0.10131642 0.09070301 0.09503507 0.09845429 0.11168735\n",
      "  0.095966   0.07820329 0.09097861 0.13156992]\n",
      " [0.10300536 0.09979062 0.09618095 0.09746064 0.09763331 0.10531101\n",
      "  0.09942448 0.09100593 0.09626585 0.11392181]\n",
      " [0.10505623 0.10068171 0.09285746 0.09607846 0.09795269 0.10937315\n",
      "  0.09757613 0.08324666 0.09282082 0.12435672]\n",
      " [0.10423121 0.10022277 0.09412465 0.09675318 0.0977496  0.10792083\n",
      "  0.09837157 0.08625042 0.09418843 0.1201873 ]\n",
      " [0.10378453 0.09996893 0.094912   0.0970652  0.09770649 0.10688061\n",
      "  0.09881468 0.08803999 0.09505484 0.1177727 ]]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array([prediction[\"topic_weights\"] for prediction in results[\"predictions\"]])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45832a9a",
   "metadata": {},
   "source": [
    "## Inference with RecordIO Protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41035b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordio_protobuf_serializer(spmatrix):\n",
    "    import io\n",
    "    import sagemaker.amazon.common as smac\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(array=spmatrix, file=buf, labels=None)\n",
    "    buf.seek(0)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41c58d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Topic ID')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fs = 12\n",
    "df = pd.DataFrame(predictions.T)\n",
    "df.plot(kind=\"bar\", figsize=(16, 4), fontsize=fs)\n",
    "plt.ylabel(\"Topic assignment\", fontsize=fs + 2)\n",
    "plt.xlabel(\"Topic ID\", fontsize=fs + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ec2d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(ntm_predictor.endpoint_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa0f3543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.105843</td>\n",
       "      <td>0.104636</td>\n",
       "      <td>0.105410</td>\n",
       "      <td>0.103208</td>\n",
       "      <td>0.103909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.101452</td>\n",
       "      <td>0.100373</td>\n",
       "      <td>0.101191</td>\n",
       "      <td>0.099882</td>\n",
       "      <td>0.099954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.090483</td>\n",
       "      <td>0.093322</td>\n",
       "      <td>0.090943</td>\n",
       "      <td>0.096002</td>\n",
       "      <td>0.094737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.095163</td>\n",
       "      <td>0.096350</td>\n",
       "      <td>0.095381</td>\n",
       "      <td>0.097329</td>\n",
       "      <td>0.096990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098565</td>\n",
       "      <td>0.098144</td>\n",
       "      <td>0.098705</td>\n",
       "      <td>0.097563</td>\n",
       "      <td>0.097643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.111982</td>\n",
       "      <td>0.108616</td>\n",
       "      <td>0.111163</td>\n",
       "      <td>0.105710</td>\n",
       "      <td>0.107156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.095833</td>\n",
       "      <td>0.097657</td>\n",
       "      <td>0.096201</td>\n",
       "      <td>0.099368</td>\n",
       "      <td>0.098792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.077457</td>\n",
       "      <td>0.084089</td>\n",
       "      <td>0.078950</td>\n",
       "      <td>0.090215</td>\n",
       "      <td>0.087859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.090870</td>\n",
       "      <td>0.093586</td>\n",
       "      <td>0.091648</td>\n",
       "      <td>0.095714</td>\n",
       "      <td>0.094857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.132352</td>\n",
       "      <td>0.123227</td>\n",
       "      <td>0.130408</td>\n",
       "      <td>0.115008</td>\n",
       "      <td>0.118104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  0.105843  0.104636  0.105410  0.103208  0.103909\n",
       "1  0.101452  0.100373  0.101191  0.099882  0.099954\n",
       "2  0.090483  0.093322  0.090943  0.096002  0.094737\n",
       "3  0.095163  0.096350  0.095381  0.097329  0.096990\n",
       "4  0.098565  0.098144  0.098705  0.097563  0.097643\n",
       "5  0.111982  0.108616  0.111163  0.105710  0.107156\n",
       "6  0.095833  0.097657  0.096201  0.099368  0.098792\n",
       "7  0.077457  0.084089  0.078950  0.090215  0.087859\n",
       "8  0.090870  0.093586  0.091648  0.095714  0.094857\n",
       "9  0.132352  0.123227  0.130408  0.115008  0.118104"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7e65d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
